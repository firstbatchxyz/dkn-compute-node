version: "3.7"
x-logging: &logging
  logging:
    driver: json-file
    options:
      max-size: 1000m

# Environment variable definitions
x-eth-client-address: &eth_client_address ${ETH_CLIENT_ADDRESS:-} # Add your ETH_CLIENT_ADDRESS after the "-"

x-rln-environment: &rln_env
  RLN_RELAY_CONTRACT_ADDRESS: ${RLN_RELAY_CONTRACT_ADDRESS:-0xF471d71E9b1455bBF4b85d475afb9BB0954A29c4}
  RLN_RELAY_CRED_PATH: ${RLN_RELAY_CRED_PATH:-} # Optional: Add your RLN_RELAY_CRED_PATH after the "-"
  RLN_RELAY_CRED_PASSWORD: ${RLN_RELAY_CRED_PASSWORD:-} # Optional: Add your RLN_RELAY_CRED_PASSWORD after the "-"

services:
  # Compute Node
  compute:
    build: "./"
    environment:
      DKN_WALLET_SECRET_KEY: ${ETH_TESTNET_KEY}
      DKN_ADMIN_PUBLIC_KEY: "0208ef5e65a9c656a6f92fb2c770d5d5e2ecffe02a6aade19207f75110be6ae658"
      DKN_TASKS: ${DKN_TASKS}
      WAKU_URL: "http://host.docker.internal:8645"
      OLLAMA_HOST: "http://host.docker.internal"
      OLLAMA_PORT: "11434"
      OLLAMA_MODEL: ${OLLAMA_MODEL:-phi3}
      RUST_LOG: "${RUST_LOGS:-info}"
      SEARCH_AGENT_URL: "http://host.docker.internal:5000"
      SEARCH_AGENT_MANAGER: true
    depends_on:
      - nwaku # TODO: in the future we may run Waku with a different compose, so this can be removed at that step

  # Waku Node
  nwaku:
    image: harbor.status.im/wakuorg/nwaku:v0.28.0
    restart: on-failure
    ports:
      - 30304:30304/tcp
      - 30304:30304/udp
      - 9005:9005/udp
      - 127.0.0.1:8003:8003
      - 80:80 # Let's Encrypt
      - 8000:8000/tcp # WSS
      - 8645:8645 # instead of: 127.0.0.1:8645:8645
    <<:
      - *logging
    environment:
      RLN_RELAY_CRED_PASSWORD: "${RLN_RELAY_CRED_PASSWORD}"
      ETH_CLIENT_ADDRESS: *eth_client_address
      EXTRA_ARGS: "${WAKU_EXTRA_ARGS}"
      LOG_LEVEL: "${WAKU_LOG_LEVEL:-DEBUG}"
      <<:
        - *rln_env
    volumes:
      - ${CERTS_DIR:-./waku/certs}:/etc/letsencrypt/:Z
      - ./run_node.sh:/opt/run_node.sh:Z
      - ./waku/rln_tree:/etc/rln_tree/:Z
      - ./waku/keystore:/keystore:Z
    entrypoint: sh
    command:
      - /opt/run_node.sh
    # profiles: [waku] # TODO: in the future we may run Waku with a different compose, keeping this comment here

  # Ollama Container (CPU)
  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    profiles: [ollama-cpu]

  # Ollama Container (ROCM)
  ollama-rocm:
    image: ollama/ollama:rocm
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    profiles: [ollama-rocm]

  # Ollama Container (CUDA)
  ollama-cuda:
    image: ollama/ollama
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: [ollama-cuda]

  # Qdrant VectorDB for Search Agent
  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage:z
    profiles: [search-python]

  # Browser automation
  browserless:
    image: ghcr.io/browserless/chromium
    environment:
      - TOKEN=${BROWSERLESS_TOKEN}
    ports:
      - "3030:3000"
    profiles: [search-python]

  search-agent:
    image: firstbatch/dria-searching-agent:latest
    ports:
      - 5000:5000
    environment:
      AGENT_MODEL_PROVIDER: ${AGENT_MODEL_PROVIDER-Ollama}
      AGENT_MODEL: ${AGENT_MODEL:-phi3}

      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      SERPER_API_KEY: ${SERPER_API_KEY}
      BROWSERLESS_TOKEN: ${BROWSERLESS_TOKEN}

      OLLAMA_URL: http://host.docker.internal:11434
      QDRANT_URL: http://host.docker.internal:6333
      BROWSERLESS_URL: http://host.docker.internal:3000
    profiles: [search-python]

volumes:
  ollama:
