services:
  # Compute Node
  compute:
    image: "firstbatch/dkn-compute-node:latest"
    # build: "./" # use this one instead if you want to build locally
    env_file:
      - .env.compose
    network_mode: "host"
    extra_hosts:
      # for Linux, we need to add this line manually
      - "host.docker.internal:host-gateway"
    restart: "on-failure"

  # Ollama Container (CPU)
  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    profiles: [ollama-cpu]

  # Ollama Container (ROCM)
  ollama-rocm:
    image: ollama/ollama:rocm
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    profiles: [ollama-rocm]

  # Ollama Container (CUDA)
  ollama-cuda:
    image: ollama/ollama
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: [ollama-cuda]

volumes:
  ollama:
