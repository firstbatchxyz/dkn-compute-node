services:
  # Compute Node
  compute:
    image: "firstbatch/dkn-compute-node:latest"
    # build: "./" # use this one instead if you want to build locally
    environment:
      RUST_LOG: ${RUST_LOG:-none,dkn_compute=info}
      # Dria
      DKN_WALLET_SECRET_KEY: ${DKN_WALLET_SECRET_KEY}
      DKN_ADMIN_PUBLIC_KEY: ${DKN_ADMIN_PUBLIC_KEY}
      DKN_MODELS: ${DKN_MODELS}
      DKN_P2P_LISTEN_ADDR: ${DKN_P2P_LISTEN_ADDR}
      DKN_RELAY_NODES: ${DKN_RELAY_NODES}
      DKN_BOOTSTRAP_NODES: ${DKN_BOOTSTRAP_NODES}
      # Api Keys
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      SERPER_API_KEY: ${SERPER_API_KEY}
      JINA_API_KEY: ${JINA_API_KEY}
      # Ollama
      OLLAMA_HOST: ${OLLAMA_HOST}
      OLLAMA_PORT: ${OLLAMA_PORT}
      OLLAMA_AUTO_PULL: ${OLLAMA_AUTO_PULL:-true}
      # vLLM 
      VLLM_PORT: ${VLLM_PORT}
    network_mode: ${DKN_DOCKER_NETWORK_MODE:-bridge}
    extra_hosts:
      # for Linux, we need to add this line manually
      - "host.docker.internal:host-gateway"
    restart: "on-failure"

  # Ollama Container (CPU)
  ollama:
    image: ollama/ollama:latest
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    profiles: [ollama-cpu]

  # vLLM Container 
  vLLM:
    image: vllm/vllm-openai:latest
    ports:
      - 11435:11435
    volumes:
      - ~/.vllm:/root/.vllm
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      VLLM_PORT: ${VLLM_PORT}
    profiles: [vllm]

  # Ollama Container (ROCM)
  ollama-rocm:
    image: ollama/ollama:rocm
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    devices:
      - "/dev/kfd"
      - "/dev/dri"
    profiles: [ollama-rocm]

  # Ollama Container (CUDA)
  ollama-cuda:
    image: ollama/ollama
    ports:
      - 11434:11434
    volumes:
      - ~/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: [ollama-cuda]

volumes:
  ollama:
